{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experiment_1_0_0_0_0_60_nuser2000', 'experiment_0_0_0_0_1_60', 'experiment_0_0_0_0_0_60', 'experiment_1_0_0_0_0_60_nuser4000', 'experiment_0_0_0_1_0_60', 'experiment_0_1_0_0_0_60', 'experiment_2_0_0_0_0_60', 'experiment_3_0_0_0_0_60', 'experiment_4_0_0_0_0_60', 'experiment_1_0_0_0_0_60', 'experiment_4_0_0_0_0_60_backup', 'experiment_0_0_1_0_0_60', 'experiment_0_0_0_2_0_60']\n",
      "['experiment 3', 'experiment_6', 'experiment_3_0_0_0_0_30', 'experiment_1_0_0_0_0_20', 'experiment_realdata_galen', 'experiment_0_0_0_0_2_xsmall', 'experiment_1_0_0_0_0_60_nuser2000', 'experiment_1_xsmall', 'experiment_8_min_pw', 'experiment_0_0_1_0_0_20', 'experiment_0_0_0_0_1_60', 'experiment_backups', 'experiment_0_1_0_0_0_xsmall', 'experiment_8_min', 'experiment_3_xsmall', 'experiment_realdata_suicide_med', 'experiment_0_0_0_0_0_60', 'experiment_6_min', 'experiment_1', 'experiment 4', 'experiment_0_0_0_0_0_xsmall', 'experiment_1_min', 'experiment_1_0_0_0_0_30', 'experiment_realdata_useless_med', 'experiment_0_0_0_1_0_med', 'experimentbackups', 'experiment_5_xsmall', 'experiment_0_0_0_1_0_25', 'experiment_1_0_0_0_0_60_nuser4000', 'experiment_0_0_0_1_0_60', 'experiment_8_xsmall', 'experiment_3_0_0_0_0_20', 'experiment_1_min_pw', 'experiment_7_min', 'experiment_9_min', 'experiment_0_1_0_0_0_60', 'experiment_2_0_0_0_0_xsmall', 'experiment_0_0_0_1_0_xsmall', 'experiment_realdata_depression_med', 'experiment_2_0_0_0_0_60', 'experiment_0_0_1_0_0_30', 'experiment_2', 'experiment_9_xsmall', 'experiment_4_min', 'experiment_realdata_suicidal_med', 'experiment_3_min', 'experiment_0_0_0_0_0_test', 'experiment_3_0_0_0_0_xsmall', 'experiment 5', 'experiment_0_0_0_0_0_40', 'experiment_realdata_suicide', 'experiment_realdata_veitch', 'experiment_4_test', 'experiment_0_0_0_1_0_20', 'experiment_3_0_0_0_0_60', 'experiment_10_min', 'experiment_0_0_0_1_0_60_backup', 'experiment_realdata_anxiety_med', 'experiment_0_1_0_0_0_20', 'experiment_0_0_0_0_0_30', 'experiment_4_0_0_0_0_60', 'experiment_6_xsmall', 'experiment_2_0_0_0_0_20', 'experiment_0_0_0_0_0_min', 'experiment_1_0_0_0_0_med', 'experiment_realdata_depression', 'experiment_3', 'experiment_0_0_1_0_0_xsmall', 'experiment_1_0_0_0_0_60', 'experiment_6_min_pw', 'experiment_2_min', 'experiment_5_min', 'experiment_1_0_0_0_0_xsmall', 'experiment_4_0_0_0_0_60_backup', 'experiment_0_0_1_0_0_60', 'experiment_2_xsmall', 'experiment_0_0_0_0_1_xsmall', 'experiment_0_0_0_0_1_20', 'experiment_2_0_0_0_0_30', 'experiment_1_0_0_0_0_small', 'experiment_realdata_anxiety', 'experiment_realdata_suicidal', 'experiment_realdata_useless', 'experiment_1_0_0_0_0_90', 'experiment_0_0_0_0_0_20', 'experiment_9_min_pw', 'experiment_0_0_0_2_0_60', 'experiment_0_1_0_0_0_30']\n"
     ]
    }
   ],
   "source": [
    "# get experimental directories\n",
    "data_directory = '/projects/bdata2/galen/causal_backups/'\n",
    "\n",
    "exp_dirs = [name for name in os.listdir(data_directory) if ('_60' in name and name[:3] == 'exp' and os.path.isdir(data_directory+name))] # or ('_40' in name and os.path.isdir(name)) or ('_60' in name and os.path.isdir(name))]\n",
    "all_exp_dirs = [name for name in os.listdir(data_directory) if (name[:3] == 'exp' and os.path.isdir(data_directory+name))] # or ('_40' in name and os.path.isdir(name)) or ('_60' in name and os.path.isdir(name))]\n",
    "\n",
    "exp_dirs.remove('experiment_0_0_0_1_0_60_backup')\n",
    "\n",
    "# get experiment features\n",
    "get_features = lambda d: [int(v) for v in d.split('_')[1:6]]\n",
    "\n",
    "# get size\n",
    "get_size = lambda d: d.split('_')[6]\n",
    "\n",
    "print(exp_dirs)\n",
    "print(all_exp_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "axis_0 = {'name': 'Linguistic Complexity',\n",
    "          'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "          'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1., 'mse_ipsw':0.},\n",
    "          'labels': ['constant','+sick','+isolation', '+death' ],\n",
    "          \n",
    "          'experiments':[\n",
    "              {'file_path': 'experiment_0_0_0_0_0_60',\n",
    "               'axis_value': 0,#0,\n",
    "               'Y_sample_classes': [[0.9,0.1],[0.9,0.9]],\n",
    "               'P_sample_classes': [0.9,0.1],\n",
    "              'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "              'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1.}\n",
    "              },\n",
    "              \n",
    "              {'file_path': 'experiment_1_0_0_0_0_60',\n",
    "               'axis_value': 1,#1,\n",
    "               'Y_sample_classes': [[0.9,0.1],[0.9,0.9]],\n",
    "               'P_sample_classes': [0.9,0.1],\n",
    "              'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "              'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1.}},\n",
    "              \n",
    "              {'file_path': 'experiment_2_0_0_0_0_60',\n",
    "               'axis_value': 2,#2,\n",
    "               'Y_sample_classes': [[0.9,0.1],[0.9,0.9]],\n",
    "               'P_sample_classes': [0.9,0.1],\n",
    "              'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "              'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1.}},\n",
    "    \n",
    "              {'file_path': 'experiment_3_0_0_0_0_60',\n",
    "               'axis_value': 3,#3,\n",
    "               'Y_sample_classes': [[0.9,0.1],[0.9,0.9]],\n",
    "               'P_sample_classes': [0.9,0.1],\n",
    "              'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "              'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1.}},\n",
    "\n",
    "              {'file_path': 'experiment_4_0_0_0_0_60_backup',\n",
    "               'axis_value': 4,\n",
    "               'Y_sample_classes': [[0.9,0.1],[0.9,0.9]],\n",
    "               'P_sample_classes': [0.9,0.1],\n",
    "              'unadjusted':{'test_acc':0.5, 'bias_strat':-0.32, 'bias_ipw':-0.32,'spearman':0.},\n",
    "              'optimal':{'test_acc':0.9, 'bias_strat':0., 'bias_ipw':0.,'spearman':1.}},\n",
    "              \n",
    "          ]}\n",
    "\n",
    "axis_0['experiments'] = axis_0['experiments'][:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = axis_0['experiments'][0]['file_path']\n",
    "Y_sample_classes = axis_0['experiments'][0]['Y_sample_classes'] #[[0.9,0.1],[0.9,0.9]]\n",
    "P_sample_classes = axis_0['experiments'][0]['P_sample_classes']\n",
    "\n",
    "inds_dict = np.load(data_directory + exp_name + '/inds_dict.npy', allow_pickle=True).item()\n",
    "_, _, T, Y, classes = np.load(data_directory+exp_name +'/data.npy', allow_pickle=True)\n",
    "P_true = P_sample_classes[0]*(1-classes) + P_sample_classes[1]*classes\n",
    "\n",
    "model_name = 'HBERT'\n",
    "stat_dict = np.load(data_directory+'{}/{}.npy'.format(exp_name, model_name), allow_pickle=True).item()\n",
    "\n",
    "P_test = stat_dict['P_test']\n",
    "Tt_test = T[inds_dict['inds_test']]\n",
    "Y_sample = np.zeros_like( P_true[inds_dict['inds_test']] )\n",
    "classes_test = classes[inds_dict['inds_test']]\n",
    "for i in range(len(classes_test)):\n",
    "    Y_sample[i] = 1.*(random.random() < Y_sample_classes[classes_test[i]][1*(Tt_test[i] == 0)])\n",
    "\n",
    "\n",
    "ATE_true = 0.5*((Y_sample_classes[0][0] - \n",
    "                 Y_sample_classes[0][1]) + \n",
    "                (Y_sample_classes[1][0] - \n",
    "                Y_sample_classes[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03198493, 0.03706409, 0.03843177, ..., 0.86624557, 0.86624569,\n",
       "       0.86624575])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(P_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effect_match(P,Z,Y, replacement=True, caliper=None, warning_tol=.1):\n",
    "    '''\n",
    "    full matching = each (or some?) control matched to one or more treated subjects (rosenbaum, 179) *without* replacement\n",
    "    \n",
    "    our implementation is from https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11293, page 784\n",
    "    \n",
    "    caliper?\n",
    "    We enforce a propensity score caliper equal to 0.1 standard\n",
    "    deviations of the estimated distribution, which discards any treated units for whom the\n",
    "    nearest control unit is not within a suitable distance.\n",
    "    from Mozer 2018: https://arxiv.org/abs/1801.00644, page 31\n",
    "     -> does this mean the distribution of all (both treated and untreated) propensity scores?\n",
    "     \n",
    "     \n",
    "    then again, rosenbaum (p. 169) recomends starting with .2\n",
    "    \n",
    "    vary caliber in terms of the standard deviation of the propensity scores, eg. .2 for 20% of a std away\n",
    "    '''\n",
    "    if not replacement:\n",
    "        assert(False) # not implemented yet (but it should be)\n",
    "        \n",
    "    P_control = P[Z == 0]\n",
    "    Y_control = Y[Z == 0]\n",
    "    \n",
    "    P_treat = P[Z == 1]\n",
    "    Y_treat = Y[Z == 1]\n",
    "    \n",
    "    # if we match all the treated to the closest untreated with replacement, we have ATE on Treated (ATT)\n",
    "    # if we match all the untreated to the closest treated with replacement, we have ATE on Untreated (ATU?)\n",
    "    # I have no idea if this works, but let's try doing both to compute ATE\n",
    "    \n",
    "    diffs = [] # difference in outcome (treated - control)\n",
    "    dists = [] # differenence in propensity score (for caliper application)\n",
    "    \n",
    "    # todo!!! also implement this without replacement\n",
    "    for treat_index, p in enumerate(P_treat):        \n",
    "        # get match for p (closest point in control by propensity)\n",
    "        control_index = np.argmin( np.abs(P_control - p) )\n",
    "        diffs.append( Y_treat[treat_index] - Y_control[control_index] )\n",
    "        dists.append( np.abs( P_control[control_index] - p ) )\n",
    "        \n",
    "    for control_index, p in enumerate(P_control):\n",
    "        # get match for p (closest point in treat by propensity)\n",
    "        treat_index = np.argmin( np.abs(P_treat - p) )\n",
    "        diffs.append( Y_treat[treat_index] - Y_control[control_index] )\n",
    "        dists.append( np.abs( P_treat[treat_index] - p ) )\n",
    "        \n",
    "    #print(f'Average distances with caliper {caliper} are {np.average(dists):8.5f}')\n",
    "    #print(f'Propensity score STD with caliper {caliper} are {np.std(P):8.5f}')\n",
    "    #print(f'Propensity score STD on treated with caliper {caliper} are {np.std(P_treat):8.5f}')\n",
    "    #print(f'Propensity score STD with on untreated caliper {caliper} are {np.std(P_control):8.5f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # CALIPER SECTION IS AGNOSTIC TO REPLACEMENT IMPLEMENTATION\n",
    "    # if using caliper, discard invalid matches\n",
    "    if caliper is not None:        \n",
    "        Valid = dists < (caliper * P.std()) # caliper is fraction of standard deviation from the distribution\n",
    "        \n",
    "        #print(f'discarded {np.sum(-1*Valid+1)} by caliper ({100*(1-(np.sum(Valid)/Valid.size)):4.2f}%)')\n",
    "        if (1. - 1.*np.sum(Valid)/Valid.size) > warning_tol:\n",
    "            print('WARNING: discarding {} of examples, exceeds tolerance of {}'.format((1. - 1.*np.sum(Valid)/Valid.size), warning_tol))\n",
    "        \n",
    "        diffs = np.array(diffs)[Valid]\n",
    "        dists = np.array(dists)[Valid]\n",
    "        \n",
    "    #print(f'Average distances after caliper are {np.average(dists):8.5f}')\n",
    "        \n",
    "    #print(f'ATE is {np.average(diffs):8.5f}, bias is {np.average(diffs)-ATE_true:8.5f}\\n')\n",
    "    return np.average(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3615"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_effect_match(P_test, Tt_test, Y_sample, caliper=None)\n",
    "get_effect_match(P_test, Tt_test, Y_sample, caliper=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrapped_estimates = []\n",
    "for _ in range(100):\n",
    "    bootsrapped_indices = np.random.choice(len(P_test), size=len(P_test), replace=True)\n",
    "    \n",
    "    bootstrapped_estimates.append(  get_effect_match(P_test[bootsrapped_indices], Tt_test[bootsrapped_indices], Y_sample[bootsrapped_indices], caliper=.2) - ATE_true   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01595845602650833"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bootstrapped_estimates).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = np.array((.1, .1, .1, .9, .1, .9, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False,  True, False, False])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs==min(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 4]),)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(diffs==min(diffs)).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice((diffs==min(diffs)).nonzero()[0] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reddit_moderation]",
   "language": "python",
   "name": "conda-env-reddit_moderation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
